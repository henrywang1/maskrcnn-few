{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=13.10s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "#import skimage.io as io\n",
    "import pickle\n",
    "from PIL import Image, ImageOps\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "import pylab\n",
    "# pylab.rcParams['figure.figsize'] = (12.0, 10.0)\n",
    "# log_folder = \"lvis_val_cocostyle_test_3\"\n",
    "# annFile = \"/home/henrywang/github/maskrcnn-benchmark-private/datasets/lvis/lvis_v0.5_val.json_all_val\"\n",
    "annFile = \"/home/henrywang/github/maskrcnn-few/datasets/lvis/annotations/lvis_v0.5_train.json\"\n",
    "coco=COCO(annFile)\n",
    "# seg_file = \"/home/henrywang/github/maskrcnn-benchmark-private/lvis/inference/{0}/segm.json\".format(log_folder)\n",
    "# cocoDt=coco.loadRes(seg_file)\n",
    "# coco = cocoDt\n",
    "cats = coco.loadCats(coco.getCatIds())\n",
    "# all_cats=[cat['name'] for cat in cats]\n",
    "cat_r = [c[\"id\"] for c in cats if c[\"frequency\"] == \"r\"]\n",
    "cat_c = [c[\"id\"] for c in cats if c[\"frequency\"] == \"c\"]\n",
    "cat_f = [c[\"id\"] for c in cats if c[\"frequency\"] == \"f\"]\n",
    "# print(len(cat_r))\n",
    "# print(len(cat_c))\n",
    "# print(len(cat_f))\n",
    "# class_names = [\"BG\"] + all_cats\n",
    "# #prepare support image\n",
    "# pair_df_file = \"/home/henrywang/github/maskrcnn-benchmark-private/lvis/inference/{0}/all_pair_df.pickle\".format(log_folder)\n",
    "# with open(pair_df_file, \"rb\") as f:\n",
    "#     pair_df = pickle.load(f)\n",
    "# annFile_s = \"/home/henrywang/github/maskrcnn-benchmark-private/datasets/lvis/lvis_v0.5_train.json\"\n",
    "# coco_s=COCO(annFile_s)\n",
    "# all_support_anns = coco_s.dataset['annotations']\n",
    "# all_support_anns = [ann for ann in all_support_anns if ann[\"category_id\"] not in cat_f]\n",
    "# annFile_s_f = \"/home/henrywang/github/maskrcnn-benchmark-private/datasets/lvis/lvis_v0.5_val.json_freq\"\n",
    "# coco_sf=COCO(annFile_s_f)\n",
    "# all_support_anns_f = coco_sf.dataset['annotations']\n",
    "\n",
    "# all_support_anns_f = [ann for ann in all_support_anns_f if ann[\"category_id\"] in cat_f]\n",
    "# all_support_anns = all_support_anns + all_support_anns_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model(\"cc.en.300.bin\")\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection, neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection, neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/henrywang/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz from cache at /home/henrywang/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350\n",
      "| dictionary: 50264 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (decoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerSentenceEncoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.base')\n",
    "roberta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = roberta.encode(\"hello world\")\n",
    "# #ret = roberta.extract_features(tokens)\n",
    "# ret = roberta.extract_features(tokens, return_all_hiddens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cat_def)\n",
    "# for i in range(7, 100):\n",
    "#     print(i)\n",
    "#     roberta.extract_features_aligned_to_words(cat_def[:i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cd47d1762241e4b8f659c5f2113930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1230), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ac99e4bff04e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mnbrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mrare_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_r\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/lvis/lib/python3.7/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'precomputed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \"\"\"\n\u001b[0;32m--> 930\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/lvis/lib/python3.7/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/lvis/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tokens = []\n",
    "labels = []\n",
    "sentences = []\n",
    "def get_sentence_vec(sentence):\n",
    "    sentence = roberta.encode(sentence)#[1:-1]\n",
    "    tok_in_sentence = roberta.extract_features(sentence, return_all_hiddens=False)\n",
    "    tok_in_sentence = tok_in_sentence.mean(0)\n",
    "    #tok_in_sentence = []\n",
    "    #for tok in doc:\n",
    "    #    tok_in_sentence.append(tok.vector)\n",
    "    #tok_in_sentence = torch.stack(tok_in_sentence)\n",
    "    return tok_in_sentence.max(0)[0].detach().cpu().numpy()\n",
    "\n",
    "for cat in tqdm(cats):\n",
    "    name = cat[\"name\"]\n",
    "    input_name = re.sub(r'\\([^)]*\\)', '', name)\n",
    "    input_name = input_name.replace(\"-\", \"\")\n",
    "    input_name = input_name.replace(\"_\", \"\")\n",
    "    #if name in [\"heart\", \"Bible\", \"racket\", \"gargle\", \"chime\", \"chap\"]:\n",
    "    #    continue\n",
    "    tokens.append(model[input_name])\n",
    "    labels.append(name)\n",
    "    cat_def = cat['def']\n",
    "    #cat_def = name + \" is a \" + cat['def']\n",
    "    #cat_def = re.sub(r'\\([^)]*\\)', '', cat_def)\n",
    "    #sentences.append(get_sentence_vec(cat_def))\n",
    "    #re.sub(r'\\([^)]*\\)', '', cat_def))\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "#X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "X = sentences\n",
    "nbrs = NearestNeighbors(n_neighbors=4, algorithm='auto').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "rare_indices = [i for i in indices if i[0] in cat_r]\n",
    "for idx in rare_indices:\n",
    "    i, j, k, l = idx[0], idx[1], idx[2], idx[3]\n",
    "    print(labels[i],labels[j],labels[k],labels[l])\n",
    "    \n",
    "# tokens = np.stack(tokens, 0)\n",
    "sentences = np.stack(sentences, 0)\n",
    "label_word_vector = np.concatenate([tokens, sentences], 1)\n",
    "#with open(\"label_word_vector.npy\", \"wb\") as f:\n",
    "#    np.save(f, label_word_vector)\n",
    "with open(\"../sentence.npy\", \"wb\") as f:\n",
    "    np.save(f, sentences)\n",
    "# with open(\"sentences.npy\", \"wb\") as f:\n",
    "#     np.save(f, sentences)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# # t-SNE embedding of the digits dataset\n",
    "# print(\"Computing t-SNE embedding\")\n",
    "# tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "# new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "# x = []\n",
    "# y = []\n",
    "# for value in new_values:\n",
    "#     x.append(value[0])\n",
    "#     y.append(value[1])\n",
    "\n",
    "# plt.figure(figsize=(16, 16)) \n",
    "# for i in range(len(x)):\n",
    "#     plt.scatter(x[i],y[i])\n",
    "#     plt.annotate(labels[i],\n",
    "#                  xy=(x[i], y[i]),\n",
    "#                  xytext=(5, 2),\n",
    "#                  textcoords='offset points',\n",
    "#                  ha='right',\n",
    "#                  va='bottom')\n",
    "# plt.show()\n",
    "# t0 = time()\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# plot_embedding(X_tsne,\n",
    "#                \"t-SNE embedding of the digits (time %.2fs)\" %\n",
    "#                (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../sentence.npy\", \"wb\") as f:\n",
    "    np.save(f, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8380257 ,  0.70382833,  0.6392304 ,  0.93795705,  2.0242496 ,\n",
       "        1.0378857 ,  0.4962853 ,  0.8408801 ,  0.7117944 ,  0.7450564 ,\n",
       "        0.8523684 ,  1.4406984 ,  0.6119056 ,  1.1649458 ,  1.1581762 ,\n",
       "        1.7479018 ,  1.6535625 ,  0.8410003 ,  0.744452  ,  1.6685802 ,\n",
       "        0.5354546 ,  1.0056696 ,  0.6451208 ,  0.69682765,  0.897118  ,\n",
       "        0.6293883 ,  0.9841778 ,  0.6805962 ,  0.8640023 ,  0.9113609 ,\n",
       "        0.58985335,  0.6807266 ,  0.6392882 ,  0.5649124 ,  0.79206353,\n",
       "        0.843032  ,  0.8399112 ,  0.71630526,  1.5858862 ,  0.527021  ,\n",
       "        1.0416435 ,  1.5366722 ,  0.5872512 ,  0.62782735,  0.42649776,\n",
       "        0.764025  ,  0.96305764,  1.2398968 ,  0.6326084 ,  0.5805859 ,\n",
       "        0.6045523 ,  0.72788894,  0.67493784,  0.5693448 ,  0.6952214 ,\n",
       "        0.85382724,  0.70938724,  1.1668227 ,  0.8411013 ,  0.65015197,\n",
       "        0.8367047 ,  3.1642177 ,  0.7423493 ,  1.255122  ,  0.52319443,\n",
       "        0.48037937,  0.58640724,  1.2238822 ,  0.8153315 ,  0.61853427,\n",
       "        0.55629665,  0.98544306,  0.5703301 ,  0.8836167 ,  0.58488166,\n",
       "        1.100131  ,  0.82444954,  0.5713902 ,  1.177469  ,  0.85441756,\n",
       "        0.6878959 ,  0.6962801 ,  2.2928734 ,  0.86734265,  0.68137056,\n",
       "        1.1888273 ,  0.61168927,  1.0642906 ,  0.6877689 ,  0.5353203 ,\n",
       "        0.86451364,  0.60564107,  0.67380106,  1.0680872 ,  0.73550916,\n",
       "        1.185795  ,  0.5436317 ,  2.9601364 ,  0.4520002 ,  0.75705814,\n",
       "        1.1891437 ,  0.5571762 ,  1.3971032 ,  0.738395  ,  0.59550893,\n",
       "        1.1999145 ,  0.9201081 ,  0.66806316,  0.584095  ,  0.6042466 ,\n",
       "        0.6582515 ,  1.040074  ,  0.9324539 ,  0.6513059 ,  0.6799824 ,\n",
       "        1.1375031 ,  0.70642877,  0.69427174,  0.5890815 ,  1.3917422 ,\n",
       "        0.6650249 ,  0.6612185 ,  1.2675035 ,  1.0267849 ,  1.1158271 ,\n",
       "        1.1314836 ,  0.69906384,  0.63665104,  0.60004133,  0.6315707 ,\n",
       "        0.5744801 ,  1.2326223 ,  0.7036022 ,  1.2168145 ,  0.863615  ,\n",
       "        0.4641024 ,  0.74234855,  0.7885705 ,  0.6523006 ,  0.66809297,\n",
       "        0.8604893 ,  0.72985613,  0.9484835 ,  0.5899159 ,  1.6243653 ,\n",
       "        0.64009845,  0.5491538 ,  0.64061904,  0.8322429 ,  0.9324016 ,\n",
       "        0.7197128 ,  0.6652118 ,  0.64526176,  0.6010167 ,  0.9610063 ,\n",
       "        1.2383372 ,  1.0062137 ,  1.5080391 ,  0.9497538 ,  2.073155  ,\n",
       "        0.765987  ,  1.2611822 ,  0.5994177 ,  0.9556471 ,  0.50253755,\n",
       "        0.5645446 ,  0.81990117,  0.6821793 ,  0.5612141 ,  1.1762568 ,\n",
       "        0.66028464,  0.7419765 ,  0.65660775,  0.5886882 ,  0.5972112 ,\n",
       "        0.6413108 ,  0.7740836 ,  0.80978954,  0.62788016,  0.67070353,\n",
       "        0.83031774,  0.8045946 ,  0.67998624,  0.6442465 ,  0.9593634 ,\n",
       "        1.2220765 ,  0.66218084,  0.9944407 ,  0.61499125,  0.8347238 ,\n",
       "        0.7274416 ,  0.69382894,  0.6621211 ,  1.6257172 ,  0.73288774,\n",
       "        0.7862286 ,  0.6348725 ,  0.4992641 ,  0.7504876 ,  0.44904673,\n",
       "        0.63460356,  0.961558  ,  0.8008166 ,  0.5882444 ,  1.3932773 ,\n",
       "        0.8049509 ,  1.4683346 ,  0.5683652 ,  0.59180903,  0.5799024 ,\n",
       "        0.5808491 ,  1.3640298 ,  0.5802962 ,  1.0137165 ,  1.1905518 ,\n",
       "        0.78394973,  0.7179818 ,  1.5724423 ,  1.0036749 ,  2.035998  ,\n",
       "        0.903871  ,  1.0335213 ,  0.86942315,  0.7839676 ,  1.2624596 ,\n",
       "        0.9739069 ,  0.7612767 ,  0.47663394,  0.6507809 ,  0.8754979 ,\n",
       "        0.6054315 ,  0.76905584,  1.2018007 ,  0.7497614 ,  0.6476036 ,\n",
       "        1.4194723 ,  0.4938886 ,  0.6632749 ,  0.7257389 ,  0.4884879 ,\n",
       "        1.9156965 ,  0.829293  ,  0.6117564 ,  0.8688925 ,  0.5379918 ,\n",
       "        1.2579349 ,  0.58320963,  2.4957201 ,  0.8789047 ,  0.78637177,\n",
       "        0.7000871 ,  0.9149206 ,  0.6607278 ,  0.5671976 ,  1.1088288 ,\n",
       "        0.5613464 ,  0.4500784 ,  0.5314963 ,  0.9519284 ,  1.7837454 ,\n",
       "        0.6835619 ,  0.7638545 ,  0.9956707 ,  0.5947763 ,  0.687925  ,\n",
       "        1.2724372 ,  0.6580172 ,  0.62023544,  0.6121596 ,  0.45995584,\n",
       "        0.6655416 ,  0.55151427,  0.7827469 ,  0.7872603 ,  0.59118485,\n",
       "        0.6375447 ,  1.57178   ,  0.5683198 ,  1.3428214 ,  0.87243146,\n",
       "        0.96447724,  0.60950077,  1.0982922 ,  0.7267583 ,  0.7949938 ,\n",
       "        0.88189065,  1.1998224 ,  0.84601396,  0.6075631 ,  0.49482593,\n",
       "        0.70869625,  0.59316635,  0.77693135,  0.6406921 ,  0.5509041 ,\n",
       "        0.71273667,  0.6293804 ,  1.1275444 ,  0.6285554 ,  0.96410817,\n",
       "        0.9360886 ,  0.68092287,  1.0758467 ,  0.7023952 ,  0.63960063,\n",
       "        0.48837608,  0.970298  ,  0.65981525,  0.6868266 ,  0.82147783,\n",
       "        0.6168128 ,  0.8247958 ,  0.53416395,  0.7244406 ,  0.5962339 ,\n",
       "        0.60943466,  0.9638486 ,  0.7230116 ,  0.7263965 ,  0.48040757,\n",
       "        0.61184156,  0.57179624,  0.6120176 ,  0.58575875,  0.9941994 ,\n",
       "        0.6970303 ,  0.9098833 ,  0.8678614 ,  0.6677753 ,  0.8365603 ,\n",
       "        3.2782843 ,  2.9824233 ,  0.77122486,  1.5661272 ,  0.660024  ,\n",
       "        0.9496578 ,  0.61375034,  0.64565504,  0.7065262 ,  0.780896  ,\n",
       "        0.76963854,  0.7361111 ,  0.712659  ,  0.49312457,  0.4611967 ,\n",
       "        0.87652737,  0.7743081 ,  0.7327644 ,  0.8754544 ,  0.7191116 ,\n",
       "        0.91450346,  0.6713173 ,  1.0551356 ,  0.9575762 ,  0.5024596 ,\n",
       "        1.0621241 ,  0.7890559 ,  0.61156803,  0.543534  ,  1.1336075 ,\n",
       "        1.0794274 ,  1.1054322 ,  0.8542999 ,  0.9116049 ,  0.5475332 ,\n",
       "        0.9396335 ,  1.3422592 ,  0.76299363,  0.56051284,  0.70869315,\n",
       "        0.70163697,  0.7460579 ,  0.5164739 ,  1.0514412 ,  0.49529693,\n",
       "        0.74545604,  0.91600996,  0.76188076,  1.6630977 ,  0.79366106,\n",
       "        0.5633647 ,  0.56908447,  0.44675055,  0.5932581 ,  0.6730665 ,\n",
       "        0.82697463,  0.8183952 ,  0.9645612 ,  1.2602143 ,  0.5507043 ,\n",
       "        0.60377055,  0.6520444 ,  0.79950625,  0.7453984 ,  0.72692   ,\n",
       "        0.5561997 ,  0.9666916 ,  2.5605915 ,  1.2632132 ,  0.47506928,\n",
       "        0.43907213,  0.9889844 ,  0.90997845,  1.4648104 ,  0.7669466 ,\n",
       "        1.1875584 ,  1.0611556 ,  0.6234575 ,  0.94475317,  1.8421533 ,\n",
       "        0.62409365,  0.73657906,  0.38100582,  0.7327938 ,  0.82627445,\n",
       "        0.5352131 ,  0.88854665,  1.4822296 ,  1.1062452 ,  0.5758395 ,\n",
       "        0.70407045,  0.5130618 ,  0.6169626 ,  0.5160989 ,  1.3317784 ,\n",
       "        0.6846406 ,  0.82705194,  0.62851757,  1.1661139 ,  0.5332324 ,\n",
       "        0.5935378 ,  0.7787473 ,  1.0253032 ,  0.6456086 ,  0.68733644,\n",
       "        0.5650627 ,  0.5416936 ,  0.4800759 ,  0.6279253 ,  0.624881  ,\n",
       "        0.89547515,  0.8182125 ,  0.96493727,  0.75893843,  0.51187   ,\n",
       "        0.55351764,  0.9380183 ,  0.59223884,  0.863135  ,  0.85882044,\n",
       "        0.8012534 ,  0.9306359 ,  0.9452385 ,  2.2027948 ,  0.899326  ,\n",
       "        0.7235399 ,  0.7547091 ,  0.6885187 ,  0.5014842 ,  0.75735563,\n",
       "        0.70847064,  0.63755214,  0.53947395,  0.459724  ,  0.5227027 ,\n",
       "        0.5974512 ,  0.45637742,  0.9485015 ,  0.9653418 ,  0.6026767 ,\n",
       "        0.6750679 ,  0.5449327 ,  0.60742784,  0.5636118 ,  0.7703828 ,\n",
       "        0.59742785,  0.88453174,  0.9378074 ,  0.6207915 ,  0.5870743 ,\n",
       "        0.5260378 ,  0.94763243,  0.556923  ,  0.3976902 ,  0.49050578,\n",
       "        0.665446  ,  0.7027939 ,  0.8471544 ,  0.7553828 ,  0.51076645,\n",
       "        0.9329508 ,  1.0017499 ,  0.91061985,  0.7838343 ,  0.22672251,\n",
       "        0.75922984,  0.77868825,  1.0665258 ,  1.4994491 ,  0.54035884,\n",
       "        0.42175356,  0.54006094,  0.79862994,  1.3760197 ,  0.47291   ,\n",
       "        0.99860805,  0.5527364 ,  0.46663076,  0.77682626,  0.84942925,\n",
       "        0.6113956 ,  1.0546249 ,  0.75878185,  0.5004138 ,  0.508578  ,\n",
       "        0.5963002 ,  0.7848299 ,  0.8363881 ,  1.3677468 ,  1.752928  ,\n",
       "        0.48431876,  0.6725386 ,  0.56600803,  0.7051222 ,  0.3935725 ,\n",
       "        0.6475667 ,  0.8208467 ,  0.5906799 ,  0.7038593 ,  1.3717079 ,\n",
       "        1.0888306 ,  0.47482777,  0.7973528 ,  0.6955131 ,  0.586363  ,\n",
       "        0.68229973,  0.84165716,  0.7053448 ,  1.4477341 ,  0.81578726,\n",
       "        0.9995426 ,  0.944787  ,  0.73794657,  0.7525728 ,  0.81987727,\n",
       "        0.5888207 ,  1.1485128 ,  0.622768  ,  0.5548601 ,  0.6616564 ,\n",
       "        0.69315785,  2.0874429 ,  0.5859156 ,  0.73548424,  0.6523852 ,\n",
       "        0.4761246 ,  0.5704878 ,  0.6515346 ,  0.59277254,  0.8679121 ,\n",
       "        0.62966275,  2.129634  ,  0.75383556,  0.6062392 ,  1.336146  ,\n",
       "        0.9176668 ,  0.6168746 ,  0.72616005,  0.832813  ,  0.6324722 ,\n",
       "        2.5994124 ,  0.7167131 ,  0.79070294,  1.6941624 ,  0.8065844 ,\n",
       "        0.55064845,  0.52398306,  1.7627922 ,  0.796392  ,  0.7255324 ,\n",
       "        1.0742726 ,  0.7851929 ,  0.5993816 ,  1.0587597 ,  0.632939  ,\n",
       "        0.9878871 ,  1.2433108 ,  0.821124  , 15.298547  ,  0.5609107 ,\n",
       "        0.519665  ,  0.83838934,  0.8395364 ,  0.67712927,  0.67625076,\n",
       "        0.88479626,  0.65890706,  0.8334092 ,  0.59882426,  0.9306682 ,\n",
       "        0.7146542 ,  0.5426088 ,  0.7206737 ,  0.90120625,  0.84666383,\n",
       "        0.7036663 ,  0.86724627,  0.57223606,  0.83654636,  1.203135  ,\n",
       "        0.5784558 ,  2.3851755 ,  0.5907522 ,  1.0153817 ,  0.9021937 ,\n",
       "        0.8151645 ,  0.4989792 ,  0.68019605,  0.66806287,  1.2034585 ,\n",
       "        0.66681767,  0.8501489 ,  0.8223651 ,  1.4995788 ,  1.852105  ,\n",
       "        0.7856862 ,  0.6905373 ,  0.9353861 ,  0.6097463 ,  0.85324264,\n",
       "        0.5349044 ,  0.52102894,  0.5644069 ,  0.6390079 ,  1.2438877 ,\n",
       "        0.7006938 ,  0.7667489 ,  0.72672486,  0.64792013,  0.53972715,\n",
       "        0.6865107 ,  0.8933373 ,  0.7105985 ,  1.0753399 ,  0.66299474,\n",
       "        0.74020976,  0.69251853,  1.5402279 ,  0.7595626 ,  0.7410911 ,\n",
       "        0.48650947,  0.91618526,  1.5595425 ,  0.85587126,  0.7967498 ,\n",
       "        1.2076834 ,  1.5275121 ,  0.4597888 ,  0.7456327 ,  0.71946824,\n",
       "        0.63704383,  0.7164014 ,  1.0060923 ,  0.84029675,  2.1866229 ,\n",
       "        0.4715171 ,  0.42728746,  1.1080376 ,  0.9748425 ,  1.0298363 ,\n",
       "        1.1778746 ,  0.84955156,  1.6734093 ,  0.7303739 ,  1.1280422 ,\n",
       "        0.79999745,  1.1026026 ,  0.69792557,  0.7647885 ,  1.2484481 ,\n",
       "        0.6825787 ,  0.5984122 ,  0.769514  ,  0.64008105,  0.87860745,\n",
       "        0.5779717 ,  0.83392924,  1.0215876 ,  0.76314235,  1.0407304 ,\n",
       "        0.6951632 ,  0.9110699 ,  0.6334267 ,  0.63237476,  0.980653  ,\n",
       "        0.8686198 ,  0.7497983 ,  0.60392535,  0.4794148 ,  0.77895   ,\n",
       "        0.9813963 ,  0.84396553,  0.8784566 ,  1.067049  ,  0.61217076,\n",
       "        0.78538734,  0.58789843,  0.799708  ,  0.5011512 ,  0.61115867,\n",
       "        0.6393474 ,  0.5978275 ,  0.69877946,  0.67729485,  0.6093963 ,\n",
       "        0.69025046,  0.6450254 ,  0.67788357,  1.3183999 ,  0.8294531 ,\n",
       "        1.6544814 ,  0.95024407,  0.63695824,  0.79933757,  0.9362263 ,\n",
       "        0.6920996 ,  1.1783055 ,  0.8905379 ,  0.98270446,  0.5914636 ,\n",
       "        0.7597702 ,  1.423717  ,  0.86776984,  1.1513233 ,  0.85518205,\n",
       "        1.4331577 ,  1.1019742 ,  0.6128002 ,  0.76389146,  0.7306995 ,\n",
       "        0.924269  ,  1.0478431 ,  0.74878025,  0.68196523,  0.5648201 ,\n",
       "        0.93173295,  0.55656695,  0.6674064 ,  0.9829615 ,  1.7570394 ,\n",
       "        0.96481097,  1.5380628 ,  1.7772578 ,  0.5101563 ,  0.6969888 ,\n",
       "        0.58645487,  0.91234255,  0.5875089 ,  0.564051  ,  0.5167344 ,\n",
       "        1.0601907 ,  0.61229223,  0.67291605,  0.7269734 ,  1.2418339 ,\n",
       "        1.2406951 ,  0.6120342 ,  0.7177567 ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Computing t-SNE embedding\")\n",
    "# tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "# t0 = time()\n",
    "# X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(model[\"apple\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cdae4c56974d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "\n",
    "            #similarity[s][ss] = 1\n",
    "        #else:\n",
    "        #    similarity[s][ss] = 0\n",
    "\n",
    "# print(\"done\")\n",
    "# def draw_text_graph(G):\n",
    "#     plt.figure(figsize=(18,12))\n",
    "#     pos = nx.spring_layout(G, scale=18)\n",
    "#     nx.draw_networkx_nodes(G, pos, node_color=\"white\", linewidths=3, node_size=500)\n",
    "#     nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "#     nx.draw_networkx_edges(G, pos)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "\n",
    "#assert False\n",
    "# for i, s in enumerate(tqdm(all_synsets)):\n",
    "#     if s == \"stop_sign.n.01\":\n",
    "#         continue\n",
    "#     x = wn.synset(s)\n",
    "#     for ss in all_synsets[i+1:]:\n",
    "#         #print(ss)\n",
    "#         if ss == \"stop_sign.n.01\":\n",
    "#             continue\n",
    "#         y = wn.synset(ss)\n",
    "#         similarity[s][ss] = x.path_similarity(y)\n",
    "    #if not s in seen:\n",
    "        #seen.add(s)\n",
    "        #graph.add_node(s)\n",
    "#     else:\n",
    "#         print(\"{0} already in the set\".format(s))\n",
    "#     #print(s)\n",
    "#     try:\n",
    "#         syn = wn.synset(s)\n",
    "#         #print(s, syn.lemmas())\n",
    "#         #assert False\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#     for ss in syn.lemmas():\n",
    "#         if not ss in seen:\n",
    "#             print(ss.name(), ss._synset._name)\n",
    "#             if ss._synset._name in all_synsets:\n",
    "#                 seen.add(ss._synset._name)\n",
    "#                 graph.add_node(ss.name())\n",
    "#                 print(\"add edge {0} to {1}\".format(ss._synset._name, s))\n",
    "#                 graph.add_edge(ss.name(), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(similarity['bear.n.01'].items(), key=lambda kv: kv[1], reverse=True)[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-280-4682dcbdb7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frequency'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'synset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'synset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'similarity' is not defined"
     ]
    }
   ],
   "source": [
    "for c in cats:\n",
    "    if c['frequency'] == 'r':\n",
    "        name = c['synset']\n",
    "        print(sorted(similarity[c['synset']].items(), key=lambda kv: kv[1], reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def wordnet_graph(words):\n",
    "    \n",
    "    \"\"\"\n",
    "    Construct a semantic graph and labels for a set of object categories using \n",
    "    WordNet and NetworkX. \n",
    "    \n",
    "    Parameters: \n",
    "    ----------\n",
    "    words : set\n",
    "        Set of words for all the categories. \n",
    "        \n",
    "    Returns: \n",
    "    -------\n",
    "    graph : graph\n",
    "        Graph object containing edges and nodes for the network. \n",
    "    labels : dict\n",
    "        Dictionary of all synset labels. \n",
    "    \"\"\"\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "    labels = {}\n",
    "    seen = set()\n",
    "    \n",
    "    def recurse(s):\n",
    "        \"\"\" Recursively move up semantic hierarchy and add nodes / edges \"\"\"  \n",
    "\n",
    "        if not s in seen:                               # if not seen...\n",
    "            seen.add(s)                                 # add to seen\n",
    "            graph.add_node(s.name())                      # add node\n",
    "            labels[s.name] = s.name().split(\".\")[0]     # add label\n",
    "            hypernyms = s.hypernyms()                   # get hypernyms\n",
    "\n",
    "            for s1 in hypernyms:                        # for hypernyms\n",
    "                graph.add_node(s1.name())                 # add node\n",
    "                graph.add_edge(s.name(), s1.name())         # add edge between\n",
    "                recurse(s1)                             # do so until top\n",
    "\n",
    "    # build network containing all categories          \n",
    "    for word in words:                                  # for all categories\n",
    "        s = wn.synset(str(word) + str('.n.01'))         # create synset            \n",
    "        recurse(s)                                      # call recurse\n",
    "\n",
    "    # return the graph and labels    \n",
    "    return graph , labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.drawing.nx_agraph import graphviz_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pylab as plt\n",
    "# from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "# G.add_node(1,level=1)\n",
    "# G.add_node(2,level=2)\n",
    "# G.add_node(3,level=2)\n",
    "# G.add_node(4,level=3)\n",
    "\n",
    "# G.add_edge(1,2)\n",
    "# G.add_edge(1,3)\n",
    "# G.add_edge(2,4)\n",
    "graph, labels = wordnet_graph(categories)\n",
    "def draw_text_graph(G):\n",
    "    plt.figure(figsize=(18,12))\n",
    "    pos = nx.spring_layout(G, scale=18)\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=\"white\", linewidths=3, node_size=500)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "    nx.draw_networkx_edges(G, pos)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "draw_text_graph(graph)\n",
    "# nx.draw(graph)\n",
    "#pos=nx.graphviz_layout(graph)\n",
    "# pos = nx.spring_layout(graph)\n",
    "# index = nx.betweenness_centrality(graph)\n",
    "# node_size = [index[n]*1000 for n in graph]\n",
    "\n",
    "#nx.draw_networkx(graph, pos, node_size=node_size, edge_color='r', alpha=.3, linewidths=0)\n",
    "\n",
    "# nx.draw(graph, pos=graphviz_layout(graph), node_size=1600, cmap=plt.cm.Blues,\n",
    "#         node_color=range(len(graph)),\n",
    "#         prog='dot')\n",
    "# plt.show()\n",
    "\n",
    "#  nx.draw_graphviz(graph)\n",
    "# 6 pos=nx.graphviz_layout(graph)\n",
    "# 7 nx.draw_networkx_labels(graph, pos=pos, labels=labels)\n",
    "# 8 pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Creates a graph of synonyms using WordNet\n",
    "\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "def graph_synsets(terms, pos=wn.NOUN, depth=0):\n",
    "    \"\"\"\n",
    "    Create a networkx graph of the given terms to the given depth.\n",
    "    \"\"\"\n",
    "\n",
    "    G = nx.Graph(\n",
    "        title=\"WordNet Synsets Graph for {}\".format(\", \".join(terms)),\n",
    "        pos=pos, depth=depth,\n",
    "    )\n",
    "\n",
    "    def add_term_links(G, term, current_depth):\n",
    "        for syn in wn.synsets(term):\n",
    "            for name in syn.lemma_names():\n",
    "                G.add_edge(term, name)\n",
    "                if current_depth < depth:\n",
    "                    add_term_links(G, name, current_depth+1)\n",
    "\n",
    "    for term in terms:\n",
    "        add_term_links(G, term, 0)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "G = graph_synsets(list(categories)[0:4])\n",
    "draw_text_graph(G)\n",
    "plt.show()\n",
    "#         if args.outpath:\n",
    "#             plt.savefig(args.outpath)\n",
    "#         else:\n",
    "#             plt.show()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "\n",
    "#     # Create the argument parser\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description=\"graph synonyms for a term\",\n",
    "#     )\n",
    "\n",
    "#     parser.add_argument(\n",
    "#         '-d', '--depth', type=int, default=2, help=\"depth to extend graph\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '-o', '--outpath', type=str, default=None, help=\"file to write figure\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '-p', '--pos', type=str, default=wn.NOUN, help=\"part of speech of word(s)\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         'words', nargs=\"+\", help=\"the words to graph synonyms for\",\n",
    "#     )\n",
    "\n",
    "#     # parse the arguments\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # run the graph computation\n",
    "#     try:\n",
    "#         G = graph_synsets(args.words, args.pos, args.depth)\n",
    "#         draw_text_graph(G)\n",
    "\n",
    "#         if args.outpath:\n",
    "#             plt.savefig(args.outpath)\n",
    "#         else:\n",
    "#             plt.show()\n",
    "\n",
    "#         print(nx.info(G))\n",
    "#     except Exception as e:\n",
    "#         parser.error(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import wordnet \n",
    "# syn = wordnet.synsets('hello')[0] \n",
    "# syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_border(input_image, border, color):\n",
    "    #img = Image.open(input_image)\n",
    "    if isinstance(border, int) or isinstance(border, tuple):\n",
    "        #debug# print(input_image, border, color)\n",
    "        if not input_image.mode == 'RGB':\n",
    "            rgbimg = Image.new(\"RGB\", input_image.size)\n",
    "            rgbimg.paste(input_image)\n",
    "            input_image = rgbimg\n",
    "        bimg = ImageOps.expand(input_image, border=border, fill=color)\n",
    "    else:\n",
    "        raise RuntimeError('Border is not an integer or tuple!')\n",
    "    return bimg\n",
    "\n",
    "def combine(query, support):\n",
    "    w1, h1 = query.size\n",
    "    w2, h2 = support.size\n",
    "    if w2 > w1:\n",
    "        support.resize(w1,h2*w1/w2)\n",
    "        w2, h2 = support.size\n",
    "    new_img = Image.new('RGB', (w1, h1+h2))\n",
    "    y_offset = 0\n",
    "    new_img.paste(support, (0,0))\n",
    "    new_img.paste(query, (0,h2))\n",
    "    return new_img   \n",
    "\n",
    "def combine_images(images):\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "    new_img = Image.new('RGBA', (total_width, max_height), (255, 0, 0, 0))\n",
    "    x_offset = 0\n",
    "    for im in images:\n",
    "        new_img.paste(im, (x_offset,0))\n",
    "        x_offset += im.size[0]\n",
    "    # patch_box = Rectangle((bbox_x, bbox_y), bbox_w, bbox_h, linewidth=0, linestyle=\"dashed\", alpha=0, facecolor=\"none\")\n",
    "    return new_img\n",
    "\n",
    "def get_one_croped_image(query_img_id, target_cat=None, cids=None):\n",
    "    support_ids= pair_df[pair_df.img_id == query_img_id].support_ann_ids.values[0]\n",
    "    support_anns = [ann for ann in all_support_anns if ann[\"id\"] in support_ids]\n",
    "    if target_cat:\n",
    "        support_anns = [ann for ann in support_anns if ann[\"category_id\"] in target_cat]\n",
    "    if cids:\n",
    "        support_anns = [ann for ann in support_anns if ann[\"category_id\"] in cids]\n",
    "    support_ids = [ann[\"id\"] for ann in support_anns]\n",
    "    support_cids = [ann[\"category_id\"] for ann in support_anns]\n",
    "    cropped_boxes = []\n",
    "    for sid, support_ann in zip(support_ids, support_anns):\n",
    "        \n",
    "        if support_ann[\"category_id\"] in cat_f:\n",
    "            img = coco_sf.loadImgs(support_ann['image_id'])[0]\n",
    "            img_file = '../datasets/lvis/val2017/%s'%img['file_name']\n",
    "        else:\n",
    "            img = coco_s.loadImgs(support_ann['image_id'])[0]\n",
    "            img_file = '../datasets/lvis/train2017/%s'%img['file_name']\n",
    "        \n",
    "        I = Image.open(img_file)\n",
    "        [x, y, w, h] = support_ann[\"bbox\"]\n",
    "        box = [x, y, x+w, y+h]\n",
    "        cropped_box = I.crop(box)\n",
    "        box_size = cropped_box.size\n",
    "        long_side = max(box_size)\n",
    "        scaling_factor = 125/long_side\n",
    "        new_size = (int(box_size[0]*scaling_factor), int(box_size[1]*scaling_factor))\n",
    "        cropped_box=cropped_box.resize(new_size)\n",
    "\n",
    "        if support_ann[\"category_id\"] in cat_r:\n",
    "            color = (255,0,0)\n",
    "        elif support_ann[\"category_id\"] in cat_c:\n",
    "            color = (0,255,0)\n",
    "        else:\n",
    "            color = (0,0,255)\n",
    "        cropped_box = add_border(cropped_box, 5, color)\n",
    "        cropped_boxes.append(cropped_box)\n",
    "\n",
    "    return combine_images(cropped_boxes), support_cids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_th(result, target_cat):\n",
    "    img_score = defaultdict()\n",
    "    img_thresholds = defaultdict()\n",
    "    for item in result:\n",
    "        # consider iouThr 0.5\n",
    "        if target_cat:\n",
    "            E = [i for i in item if i[\"category_id\"] in target_cat]\n",
    "        else:\n",
    "            E = item\n",
    "        if not E:\n",
    "            continue\n",
    "        dtScores = np.concatenate([e['dtScores'] for e in E])\n",
    "        inds = np.argsort(-dtScores, kind='mergesort')\n",
    "\n",
    "        dtScoresSorted = dtScores[inds]\n",
    "        dtm  = np.concatenate([e['dtMatches'][0] for e in E])[inds]\n",
    "        dtIg = np.concatenate([e['dtIgnore'][0]  for e in E])[inds]\n",
    "        tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n",
    "        fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n",
    "        tp_sum = np.cumsum(tps).astype(dtype=np.float)\n",
    "        fp_sum = np.cumsum(fps).astype(dtype=np.float)\n",
    "\n",
    "        tp = np.array(tp_sum)\n",
    "        fp = np.array(fp_sum)\n",
    "        npig = len(tp)\n",
    "        rc = tp / npig\n",
    "        pr = tp / (fp+tp+np.spacing(1))\n",
    "        q  = np.zeros((R,))\n",
    "        ss = np.zeros((R,))\n",
    "\n",
    "        try:\n",
    "            for ri, pi in enumerate(inds):\n",
    "                q[ri] = pr[pi]\n",
    "                ss[ri] = dtScoresSorted[pi]\n",
    "        except:\n",
    "            pass\n",
    "        beta=0.5\n",
    "        f1 = ((1+beta**2)*pr*rc)/(beta**2*pr+rc)\n",
    "        if not pr.any() or not rc.any() or np.all(np.isnan(f1)):\n",
    "            continue\n",
    "\n",
    "        precision = np.array(q)\n",
    "        scores = np.array(ss)\n",
    "        mean_precision = np.mean(precision)\n",
    "        mean_precision = np.max(f1)\n",
    "        image_id = [i[\"image_id\"] for i in E][0]\n",
    "        img_score[image_id] = mean_precision\n",
    "        th = dtScoresSorted[np.nanargmax(f1)] - 0.00001\n",
    "        img_thresholds[image_id] = th\n",
    "    return img_score, img_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def save_outputs(imgIds, thresholds, root, target_cat=None):\n",
    "    if not os.path.exists(root):\n",
    "        os.makedirs(root)\n",
    "        os.makedirs(root + \"/all\")\n",
    "    images = coco.loadImgs(imgIds)\n",
    "    for img, th in zip(images, thresholds):\n",
    "        annIds = coco.getAnnIds(img['id'], iscrowd=None)\n",
    "        anns = coco.loadAnns(annIds)\n",
    "        if target_cat:\n",
    "            anns = [a for a in anns if a[\"category_id\"] in target_cat]\n",
    "        anns = [a for a in anns if a[\"score\"] > th]\n",
    "        if len(anns) == 0:\n",
    "            print(\"no annotations\")\n",
    "            continue\n",
    "        if target_cat:\n",
    "            cids = [ann[\"category_id\"] for ann in anns if ann[\"category_id\"] in target_cat]\n",
    "        else:\n",
    "            cids = [ann[\"category_id\"] for ann in anns if ann[\"category_id\"]]\n",
    "        #if (not(set(cids)&set(cat_r)) and not(set(cids)&set(cat_c))):\n",
    "        if not(set(cids)&set(cat_r)):\n",
    "            continue\n",
    "        \n",
    "        # print((set(cids)&set(cat_r)),(set(cids)&set(cat_c)))\n",
    "        #if target_cat and (not set(cids) & set(target_cat)):\n",
    "        #    print(\"no target categories\")\n",
    "        #    continue\n",
    "        if not cids:\n",
    "            print(\"no target categories\")\n",
    "            continue\n",
    "\n",
    "        if target_cat:\n",
    "            anns = [ann for ann in anns if ann[\"category_id\"] in target_cat]\n",
    "\n",
    "\n",
    "        plt.axis('off')\n",
    "        I = Image.open('../datasets/lvis/val2017/%s'%img['file_name'])\n",
    "        \n",
    "        max_size = max(I.size)\n",
    "        w, h = I.size\n",
    "        if max_size > 1000.0:\n",
    "            scale = 1000.0/max_size\n",
    "            w = int(w*scale)\n",
    "            h = int(h*scale)\n",
    "            I = I.resize(w, h)\n",
    "            w, h = I.size\n",
    "\n",
    "        # plt.autoscale(False)\n",
    "        plt.imshow(I);# plt.axis('off')\n",
    "        # draw_caption=True if len(set(cids)) > 1 else False\n",
    "        draw_caption = True\n",
    "\n",
    "        \n",
    "\n",
    "        # coco.showSupports(support)\n",
    "        coco.showAnns(anns, class_names=class_names,\n",
    "                      show_mask=True,\n",
    "                      show_bbox=True, \n",
    "                      box_width=0.5,\n",
    "                      draw_caption=draw_caption,\n",
    "                      text_size=8,\n",
    "                     )\n",
    "        supports, support_cids= get_one_croped_image(img['id'], target_cat, cids)\n",
    "        \n",
    "        ws, hs = supports.size\n",
    "        scaling_factor = h*0.2/hs\n",
    "        supports = supports.resize((int(ws*scaling_factor), int(hs*scaling_factor)))\n",
    "        plt.imshow(supports, zorder=100)\n",
    "        \n",
    "        \n",
    "        #if not target_cat:\n",
    "        folder = \"all\"\n",
    "        #else:\n",
    "        #    folder = \"rare\" if set(cids) & set(cat_r) else \"common\"\n",
    "        classes = [class_names[i] for i in support_cids]\n",
    "        classes  = [(re.sub(r\" ?\\([^)]+\\)\", \"\", c)) for c in classes]\n",
    "        classes = ','.join(classes)\n",
    "        plt.savefig('{0}/{1}/output-{2}-{3}.png'.format(root, folder, img['id'], classes), dpi=300, bbox_inches='tight',pad_inches = 0)\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_category_id_to_contiguous_id = {\n",
    "    v: i + 1 for i, v in enumerate(coco.getCatIds())\n",
    "}\n",
    "img_id = 274066\n",
    "img = coco.loadImgs(img_id)[0]\n",
    "\n",
    "I = Image.open('../datasets/lvis/val2017/%s'%img['file_name'])\n",
    "annIds = coco.getAnnIds(imgIds=img['id'], iscrowd=None)\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "length = len(anns)\n",
    "\n",
    "# break\n",
    "import copy \n",
    "_anns = copy.deepcopy(anns)\n",
    "for i, ann in enumerate(anns):\n",
    "    _anns[i][\"category_id\"] = json_category_id_to_contiguous_id[anns[i][\"category_id\"]]\n",
    "anns = _anns\n",
    "# anns = [anns[0], anns[1]]\n",
    "# load and display instance annotations\n",
    "# print(img_id)\n",
    "cids = [ann[\"category_id\"] for ann in anns]\n",
    "classes = [class_names[i] for i in list(set(cids))]\n",
    "# print(classes)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(I);# plt.axis('off')\n",
    "anns = [ann for ann in anns if ann[\"score\"]>0.3]\n",
    "for i, c in enumerate(class_names):\n",
    "    class_names[i] = class_names[i].replace(\"_\", \"\")\n",
    "    if c == \"flower_arrangement\":\n",
    "        class_names[i] = 'flower'\n",
    "#anns = [a for a in anns if not a[\"category_id\"]==1057]\n",
    "coco.showAnns(anns, class_names=class_names, show_mask=True, show_bbox=True, box_width=3, draw_caption=True, text_size=10)\n",
    "#fig = plt.figure()\n",
    "#fig.canvas.draw()\n",
    "#fig.canvas.flush_events()\n",
    "# support, _ = get_one_croped_image(img['id'])\n",
    "\n",
    "#print(support.size)\n",
    "#print(I.size)\n",
    "        #box_size = cropped_box.size\n",
    "        #long_side = max(box_size)\n",
    "        #scaling_factor = 125/long_side\n",
    "        #new_size = (int(box_size[0]*scaling_factor), int(box_size[1]*scaling_factor))\n",
    "        #cropped_box=cropped_box.resize(new_size)\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(1,1,1)\n",
    "#ax = plt.axes([0.32, 0.8, 0.1, 0.1], frameon=True)  # Change the numbers in this array to position your image [left, bottom, width, height])\n",
    "#ax = plt.gca([0.3, 0.8, 0.1, 0.1], frameon=True)\n",
    "#ax.imshow(support)\n",
    "#plt.imshow(support)\n",
    "#ax = plt.axes([0,0,1,1], anchor='NW', frameon=True)\n",
    "#ax.imshow(support)\n",
    "#offset = -0.5\n",
    "\n",
    "#plt.imshow(support, zorder=100)\n",
    "#plt.savefig('teaser/teaser_tomato.png', dpi=300, bbox_inches='tight',pad_inches = 0)\n",
    "\n",
    "#plt.savefig('teaser/teaser_flower.png', dpi=300, bbox_inches='tight',pad_inches = 0)\n",
    "plt.savefig('teaser/2-way.png', dpi=250, bbox_inches='tight',pad_inches = 0)\n",
    "#plt.savefig('teaser/teaser_2way.png', dpi=300, bbox_inches='tight',pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(cids)\n",
    "# {1057, 1200}\n",
    "# classes\n",
    "# ['wet_suit', 'surfboard']\n",
    "catIds = coco.getCatIds([\"wet_suit\", \"surfboard\"]);\n",
    "#print(catIds)\n",
    "i=7\n",
    "imgIds = coco.getImgIds(catIds=catIds );\n",
    "img = coco.loadImgs(imgIds[i])[0]\n",
    "I = Image.open('../datasets/lvis/val2017/%s'%img['file_name'])\n",
    "plt.axis('off')\n",
    "plt.imshow(I);\n",
    "plt.savefig('teaser/support.png', dpi=300, bbox_inches='tight',pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"../lvis/inference/\" + log_folder + \"/coco_evaluate_result.pkl\"\n",
    "if os.path.isfile(log_file):\n",
    "    with open(log_file, 'rb') as handle:\n",
    "        coco_evaluate_result = pickle.load(handle)\n",
    "else:\n",
    "    output_folder=\"../lvis\"\n",
    "    ann_type_id=0\n",
    "    split=0\n",
    "\n",
    "    annType = ['segm', 'bbox', 'keypoints']\n",
    "    annType = annType[ann_type_id]  # specify type here\n",
    "    prefix = 'instances'\n",
    "    dataDir = '../datasets/lvis'\n",
    "    annFile = '%s/lvis_v0.5_val.json_all' % (dataDir) # \n",
    "    cocoGt = COCO(annFile)\n",
    "    resFile = './%s/inference/%s/%s.json' % (output_folder, log_folder, annType)\n",
    "    targets_fname = \"%s/inference/%s/target.pth\" % (output_folder, log_folder)\n",
    "    if os.path.isfile(targets_fname):\n",
    "        targets = torch.load(targets_fname)\n",
    "    else:\n",
    "        targets = None\n",
    "    cocoDt = cocoGt.loadRes(resFile)\n",
    "    imgIds = sorted(cocoGt.getImgIds())\n",
    "    imgIds = imgIds\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, annType, split=split, targets=targets)\n",
    "    cocoEval.params.imgIds = imgIds\n",
    "    coco_evaluate_result = cocoEval.evaluate_debug()\n",
    "    with open(log_file, \"wb\") as handle:\n",
    "        pickle.dump(coco_evaluate_result, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cat = None\n",
    "# maxDet = 20\n",
    "recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n",
    "R = len(recThrs)\n",
    "_img_score, _img_thresholds = get_image_and_th(coco_evaluate_result, target_cat)\n",
    "img_ids = np.array([k for k, v in _img_score.items()])\n",
    "img_scores = np.array([v for k, v in _img_score.items()])\n",
    "img_thresholds = np.array([v for k, v in _img_thresholds.items()])\n",
    "inds = np.argsort(-img_scores, kind='mergesort')\n",
    "\n",
    "inds = inds[:100]\n",
    "img_thresholds = img_thresholds[inds]\n",
    "img_scores = img_scores[inds]\n",
    "test_images = img_ids[inds]\n",
    "import shutil\n",
    "img_output_folder = \"outputs_lvis\"\n",
    "shutil.rmtree(img_output_folder, ignore_errors=True, onerror=None)\n",
    "save_outputs(test_images, img_thresholds, img_output_folder, target_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reszie_one(img, max_height):\n",
    "    w, h = img.size\n",
    "    #print(max_height, max_height/h)\n",
    "    scale = (max_height)/(h)\n",
    "    scale = scale\n",
    "    #print((w*scale, h*scale))\n",
    "    return img.resize((int(w*scale), int(h*scale)))\n",
    "\n",
    "def resize_imgs(imgs):\n",
    "    for i in range(0,4):\n",
    "        left = i*4\n",
    "        right = (i+1)*4\n",
    "        max_height = max([img.size[1] for img in imgs[left:right]])\n",
    "        imgs[left:right] = [reszie_one(img, max_height) for img in imgs[left:right]]\n",
    "    return imgs\n",
    "\n",
    "def getKey(item):\n",
    "    return item.size[0]\n",
    "\n",
    "import numpy\n",
    "mypath = \"./output/\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [mypath + f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "cls_names = [f.split(\"-\")[-1].split(\".\")[0] for f in onlyfiles]\n",
    "cls_names = cls_names[:16]\n",
    "print(cls_names)\n",
    "#assert False\n",
    "#print(cls_names)\n",
    "#assert False\n",
    "#cls_names= [c.replace('_', '-') for c in cls_names]\n",
    "cls_names=[c.split(',') for c in cls_names]\n",
    "for i,c in enumerate(cls_names):\n",
    "    #print(c)\n",
    "    for j,cc in enumerate(c):\n",
    "        c[j] = cc.rstrip(\"_\")\n",
    "        c[j] = c[j].replace(\"_\", \"-\")\n",
    "        #print(c[j])\n",
    "    if len(c)>3:\n",
    "        cls_names[i] = c[:3] + [\"...\"]\n",
    "cls_names=[','.join(c) for c in cls_names]\n",
    "#imgs = [Image.open(f) for f in onlyfiles[:16]]\n",
    "imgs = [Image.open(f) for f in onlyfiles[:16]]\n",
    "widths = [img.size[0] for img in imgs]\n",
    "#print(widths)\n",
    "#max_widths = [widths[3],widths[7], widths[11],widths[15]]\n",
    "idx = numpy.argsort(widths)\n",
    "widths = numpy.sort(widths)\n",
    "max_widths = [widths[3],widths[7], widths[11],widths[15]]\n",
    "#max_widths = [widths[2],widths[5], widths[8],widths[11]]\n",
    "# print(max_widths)\n",
    "idx = idx[[0,7,8,15,\n",
    "           1,6,9,14,\n",
    "           2,5,10,13,\n",
    "           3,4,11,12]]\n",
    "# idx = idx[[0,5,6,9,\n",
    "#            1,4,7,10,\n",
    "#            2,3,8,11]]\n",
    "imgs = [imgs[i] for i in idx]\n",
    "cls_names = [cls_names[i] for i in idx]\n",
    "widths = [img.size[0] for img in imgs]\n",
    "\n",
    "#print([img.size[0] for img in imgs])\n",
    "\n",
    "\n",
    "\n",
    "#array([0, 1, 2, 4, 3])\n",
    "#sorted(imgs, key=getKey)\n",
    "\n",
    "# idx = [0,7,8,15,1,6,9,14,2,5,10,13,4,7,11,12]\n",
    "\n",
    "# imgs = resize_imgs(imgs)\n",
    "# im1 = np.arange(100).reshape((10, 10))\n",
    "# im2 = im1.T\n",
    "# im3 = np.flipud(im1)\n",
    "# im4 = np.fliplr(im2)\n",
    "\n",
    "fig = plt.figure(figsize=(16., 16.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(4, 4),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.23,  # pad between axes in inch.\n",
    "                 )\n",
    "widths = [img.size[0] for img in imgs]\n",
    "\n",
    "#print(widths)\n",
    "for i, (ax, im, kls) in enumerate(zip(grid, imgs, cls_names)):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    \n",
    "    idx=i%4\n",
    "    mw = max_widths[idx]\n",
    "    w, h = im.size\n",
    "    #extent = ((mw-w)/2-0.5, (mw+w)/2-0.5, h-0.5, -0.5)\n",
    "    extent = None\n",
    "    #left =\n",
    "    #print(w,h)\n",
    "#     print(ax.get_ylim())\n",
    "#     print(ax.get_xlim())\n",
    "#     if h>w:\n",
    "#         #left, right, bottom, top = ax.get_extent()\n",
    "        \n",
    "#         #scale = w/h\n",
    "#         left = (h-w)/2\n",
    "#         right = (h-w)/2 + w\n",
    "#         #print(left/h, right/h)\n",
    "#         #left = scale/2\n",
    "#         #right = 1-scale/2\n",
    "#         extent=(left, right, h+0.5, -0.5)\n",
    "#     else:\n",
    "#         extent=None\n",
    "    \n",
    "    _im = ax.imshow(im) #, aspect='equal', extent=extent\n",
    "    # print(_im.get_extent())\n",
    "\n",
    "        \n",
    "    ax.set_title(kls,fontsize= 10)\n",
    "    ax.axis('off')\n",
    "#extent=(0,2, 0,1)\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.axis('off')\n",
    "plt.savefig('rare.pdf', dpi=170, bbox_inches='tight',pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and visualize the embedding vectors\n",
    "# def plot_embedding(X, title=None):\n",
    "#     x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "#     X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "#     plt.figure()\n",
    "#     ax = plt.subplot(111)\n",
    "#     for i in range(X.shape[0]):\n",
    "#         plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "#                  #color=plt.cm.Set1(y[i] / 10.),\n",
    "#                  fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "#     if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "#         # only print thumbnails with matplotlib > 1.0\n",
    "#         shown_images = np.array([[1., 1.]])  # just something big\n",
    "#         for i in range(X.shape[0]):\n",
    "#             dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "#             if np.min(dist) < 4e-3:\n",
    "#                 # don't show points that are too close\n",
    "#                 continue\n",
    "#             shown_images = np.r_[shown_images, [X[i]]]\n",
    "#             #imagebox = offsetbox.AnnotationBbox(\n",
    "#             #    offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "#             #    X[i])\n",
    "#             #ax.add_artist(imagebox)\n",
    "#     plt.xticks([]), plt.yticks([])\n",
    "#     if title is not None:\n",
    "        plt.title(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
